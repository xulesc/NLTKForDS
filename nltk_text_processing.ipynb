{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'NLP', 'tasks', '.', 'It', 'can', 'handle', 'tokenization', 'effectively', '.']\n",
      "Sentences:  ['NLTK is a powerful tool for NLP tasks.', 'It can handle tokenization effectively.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = \"NLTK is a powerful tool for NLP tasks. It can handle tokenization effectively.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Words: \", words)\n",
    "print(\"Sentences: \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words:  ['NLTK', 'powerful', 'tool', 'NLP', 'tasks', '.', 'handle', 'tokenization', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filter_words = list(filter(lambda x: x.lower() not in stop_words, words))\n",
    "\n",
    "print(\"Filtered words: \", filter_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmaatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['run', 'ran', 'jump', 'jump']\n",
      "Lemmatized words: ['run', 'run', 'jump', 'jump']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['running', 'ran', 'jumps', 'jumping']\n",
    "stemmed_words = list(map(lambda x: porter_stemmer.stem(x), words))\n",
    "lemmatized_words = list(map(lambda x: lemmatizer.lemmatize(x, pos='v'), words))\n",
    "\n",
    "print('Stemmed words:', stemmed_words)\n",
    "print('Lemmatized words:', lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging:  [('running', 'VBG'), ('ran', 'VBD'), ('jumps', 'NNS'), ('jumping', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tagged_words = pos_tag(words)\n",
    "\n",
    "print(\"POS Tagging: \", tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:  (S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "\n",
    "tagged_sentence = pos_tag(word_tokenize(sentence))\n",
    "named_entities = ne_chunk(tagged_sentence)\n",
    "\n",
    "print('Named Entities: ', named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpra and Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  192427\n",
      "Number of sentences:  7752\n",
      "Average words per sentence:  24.822884416924666\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Load the gunteberg corpus\n",
    "emma = gutenberg.words('austen-emma.txt')\n",
    "\n",
    "# Find the number of words in the gunteberg corpus\n",
    "num_words = len(emma)\n",
    "\n",
    "# Find the number of sentences in the gunteberg corpus\n",
    "num_sentences = len(gutenberg.sents('austen-emma.txt'))\n",
    "\n",
    "# Calculate the average words per sentence\n",
    "avg_words_per_sentence = num_words / num_sentences\n",
    "\n",
    "print('Number of words: ', num_words)\n",
    "print('Number of sentences: ', num_sentences)\n",
    "print('Average words per sentence: ', avg_words_per_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordnet and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms of 'happy':  ['happy', 'felicitous', 'glad', 'happy']\n",
      "Hypernyms of 'dog':  [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Get synonyms of the word happy\n",
    "synsets = wordnet.synsets('happy')\n",
    "synonyms = list(map(lambda x: x.lemmas()[0].name(), synsets))\n",
    "\n",
    "# Get hypernyms for dog\n",
    "synsets_dog = wordnet.synsets('dog')\n",
    "hypernyms = synsets_dog[0].hypernyms()\n",
    "\n",
    "print(\"Synonyms of 'happy': \", synonyms)\n",
    "print(\"Hypernyms of 'dog': \", hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix\n",
      "[[0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0]\n",
      " [1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1]\n",
      " [0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0]]\n",
      "Vocabulary:  ['analysis' 'data' 'feelings' 'finds' 'for' 'helps' 'hidden' 'in' 'is'\n",
      " 'modeling' 'nlp' 'nltk' 'patterns' 'powerful' 'sentiment' 'tasks' 'tool'\n",
      " 'topic' 'understand' 'user']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"NLTK is a powerful tool for NLP tasks.\",\n",
    "    \"Sentiment analysis helps understand user feelings.\",\n",
    "    \"Topic modeling finds hidden patterns in data.\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data into a feature matrix\n",
    "feature_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Feature matrix\")\n",
    "print(feature_matrix.toarray())\n",
    "print(\"Vocabulary: \", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "\n",
    "# Prepare data for sentiment analysis\n",
    "positive_reviews = [(movie_reviews.words(fileId), 'positive') for fileId in movie_reviews.fileids('pos')]\n",
    "negative_reviews = [(movie_reviews.words(fileId), 'negative') for fileId in movie_reviews.fileids('neg')]\n",
    "reviews = positive_reviews + negative_reviews\n",
    "\n",
    "# Create feature sets using BoW representation\n",
    "def extract_features(words):\n",
    "    return dict(list(map(lambda x: (x, True), words)))\n",
    "\n",
    "feature_sets = [(extract_features(words), sentiment) for (words, sentiment) in reviews]\n",
    "\n",
    "# Split the data in training and testing sets\n",
    "train_set = feature_sets[:800]\n",
    "test_set = feature_sets[800:]\n",
    "\n",
    "# Build the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test the accuracy of the classifier\n",
    "accuracy = nltk_accuracy(classifier, test_set)\n",
    "\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score:  {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.8619}\n"
     ]
    }
   ],
   "source": [
    "# Using the VADER sentiment analysis from NLTK\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Sample text for sentiment analysis\n",
    "text = \"I love this product! It is amazing!\"\n",
    "\n",
    "# Create SentimentIntensityAnalyzer instance\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Get the sentiment score for the text\n",
    "sentiment_score = sid.polarity_scores(text)\n",
    "\n",
    "print('Sentiment score: ', sentiment_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5475\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for sentiment analysis\n",
    "positive_reviews = [(movie_reviews.raw(fileId), 'positive') for fileId in movie_reviews.fileids('pos')]\n",
    "negative_reviews = [(movie_reviews.raw(fileId), 'negative') for fileId in movie_reviews.fileids('neg')]\n",
    "reviews = positive_reviews + negative_reviews\n",
    "\n",
    "# Shuffle the reviews for training and testing sets\n",
    "import random\n",
    "random.shuffle(reviews)\n",
    "\n",
    "# Create feature sets using BoW representation\n",
    "feature_sets = [(extract_features(words), sentiment) for (words, sentiment) in reviews]\n",
    "\n",
    "# Split the data in training and testing sets\n",
    "train_set = feature_sets[:1600]\n",
    "test_set = feature_sets[1600:]\n",
    "\n",
    "# Build the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(feature_sets)\n",
    "\n",
    "# Test the classifier on the test set\n",
    "accuracy = nltk_accuracy(classifier, test_set)\n",
    "\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "feeling user analysis sentiment understand\n",
      "Topic 2:\n",
      "modeling topic data find hidden\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import inaugural, brown, reuters, gutenberg, movie_reviews, webtext, nps_chat, treebank, conll2000, names, wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "corpus = [\n",
    "    \"NLTK is a powerful tool for NLP tasks.\", \"Topic modeling finds hidden patterns in data.\",\n",
    "    \"Sentiment analysis helps understand user feelings.\", \"LDA is a popular topic modeling algorithm.\", \n",
    "    \"Natural Language Processing is an exciting field.\", \"Text classification categorizes documents in classes.\"\n",
    "]\n",
    "\n",
    "# Function to prepare the text data\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    words = list(filter(lambda x: x.isalpha(), words))\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = list(filter(lambda x: x not in stop_words, words))\n",
    "\n",
    "    # Lemmatize words\n",
    "    words = list(map(lambda x: lemmatizer.lemmatize(x), words))\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Preprocess the corpus\n",
    "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data into a feature matrix\n",
    "feature_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# Build the LDA model\n",
    "num_topics = 2\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(feature_matrix)\n",
    "\n",
    "# Display the topics\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words -1 : -1]]))\n",
    "\n",
    "num_top_words = 5\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), num_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltkfords",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
